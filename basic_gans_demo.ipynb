{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "# draw plots in notebook not new window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mengningshang/Desktop/Dev_Env/carAI/CarND-Alexnet-Feature-Extraction/wcd-ml-b2/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./MNIST_DATA/train-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/train-labels-idx1-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-images-idx3-ubyte.gz\n",
      "Extracting ./MNIST_DATA/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "'''\n",
    "switch for Fashion or Digits Data set\n",
    "'''\n",
    "mn = input_data.read_data_sets(\"./MNIST_DATA\",one_hot=True)        \n",
    "# mn = input_data.read_data_sets(\"./FASHION_DATA\",one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of image is:  (784,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x11d9f9a58>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111198860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('shape of image is: ', mn.train.images[0].shape)          # image is a long flat vector\n",
    "plt.imshow(mn.train.images[0].reshape(28,28), cmap=\"Greys\")     # reformat long flat img vector in to 28x28 square img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define generator/faker net with own var_scope\n",
    "def gen(z, reuse=None, extra_layer=0):\n",
    "    '''\n",
    "    input: takes random seed z \n",
    "    outputs: a 784 pixel img vector\n",
    "    '''\n",
    "    with tf.variable_scope('gen', reuse=reuse):\n",
    "        leaky_relu_const = 0.01 # using RELU (sparse) causes instability, use leaky relu\n",
    "        hid1 = tf.layers.dense(inputs=z, units=128)\n",
    "        hid1 = tf.maximum(leaky_relu_const*hid1, hid1)\n",
    "        hid2 = tf.layers.dense(inputs=hid1, units=128)\n",
    "        hid2 = tf.maximum(leaky_relu_const*hid2, hid2)\n",
    "        if extra_layer:\n",
    "            hid3 = tf.layers.dense(inputs=hid2, units=256)\n",
    "            hid3 = tf.maximum(leaky_relu_const*hid3, hid3)\n",
    "            outl = tf.layers.dense(hid3, units=784, activation=tf.nn.tanh)\n",
    "        else:\n",
    "            outl = tf.layers.dense(hid2, units=784, activation=tf.nn.tanh)\n",
    "        return outl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define descriminator/judge net with own var_scope\n",
    "def judge(x, reuse=None, extra_layer=0):\n",
    "    '''\n",
    "    input: a 784 pixel img vector (either generated by the faker or a real example from MNIST)\n",
    "    output: tuple (Boolean (0 or 1) where 1 = image is real (MNIST) and not generated, prediction proability)\n",
    "    '''\n",
    "    with tf.variable_scope('jud', reuse=reuse):\n",
    "        hid1 = tf.layers.dense(inputs=x, units=128)\n",
    "        leaky_relu_const = 0.01 \n",
    "        hid1 = tf.maximum(leaky_relu_const*hid1, hid1)\n",
    "        hid2 = tf.layers.dense(inputs=hid1, units=128)\n",
    "        hid2 = tf.maximum(leaky_relu_const*hid2, hid2)\n",
    "        if extra_layer:\n",
    "            hid3 = tf.layers.dense(inputs=hid2, units=256)\n",
    "            hid3 = tf.maximum(leaky_relu_const*hid3, hid3)\n",
    "            logits = tf.layers.dense(hid3, units=1)\n",
    "        else:\n",
    "            logits = tf.layers.dense(hid2, units=1)\n",
    "        outl = tf.sigmoid(logits)\n",
    "        return outl, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define placeholders\n",
    "\n",
    "extra_layer=True # use 1 for fashion\n",
    "\n",
    "real_img = tf.placeholder(tf.float32, shape=[None, 784])\n",
    "z= tf.placeholder(tf.float32, shape=[None, 100])  # random seed for the faker\n",
    "G = gen(z, extra_layer=extra_layer) # initate a faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "JoutReal, JlogitsReal = judge(real_img, extra_layer=extra_layer)  # this is a tensorFlow node, this version takes in real MNIST images\n",
    "JoutFake, JlogitsFake = judge(G, reuse=True, extra_layer=extra_layer) # this is a tensorFlow node, this version takes in faked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# losses\n",
    "def lossfunc (logitsin, yin):\n",
    "    '''\n",
    "    for each 0/1 prediction, calculate the error, and return the average for the batch\n",
    "    '''\n",
    "    return tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logitsin, labels=yin))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define labels (either 0 or 1 because we know if the image is a fake or real MNIST data)\n",
    "JlossReal = lossfunc(JlogitsReal, tf.ones_like(JlogitsReal)*0.90) \n",
    "'''\n",
    "tf.ones_like(JlogitsReal)*0.9 is applying a smoothing factor to real images (lablel = 1) \n",
    "so the model genearlize better\n",
    "'''\n",
    "JlossFake = lossfunc(JlogitsFake, tf.zeros_like(JlogitsFake))\n",
    "Jloss = JlossReal + JlossFake # overall error for the judge-net\n",
    "\n",
    "'''\n",
    "overall error for the faker net, it doesn't care about loss on real MNIST\n",
    "Is labeled 1 (a lil counter-intuitive) => since if \n",
    "'''\n",
    "Gloss=lossfunc(JlogitsFake, tf.ones_like(JlogitsFake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define visualizer function\n",
    "\n",
    "skip = 5\n",
    "\n",
    "def visualize_sample(imgarray, pos, greyscale=0):\n",
    "    img = imgarray.reshape(28,28)\n",
    "    fig = plt.figure()\n",
    "    fig.suptitle(\"after {} epochs\".format(pos), fontsize=14, fontweight='bold')\n",
    "    if greyscale == 1:\n",
    "        plt.imshow(img, cmap='Greys')\n",
    "    else: \n",
    "        plt.imshow(img)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define pipeline parameters\n",
    "\n",
    "batchsz=50 # images before we make a update\n",
    "rounds = 256 # times we train over the whole data-set\n",
    "samples = [] # a array to store the random samples at each epoch\n",
    "lr = 0.001 # learning rate\n",
    "\n",
    "# define what to optimize\n",
    "allvars = tf.trainable_variables()\n",
    "jvars = [var for var in allvars if 'jud' in var.name]\n",
    "gvars = [var for var in allvars if 'gen' in var.name]\n",
    "\n",
    "jtrain = tf.train.AdamOptimizer(lr).minimize(Jloss, var_list=jvars) # train judge only\n",
    "gtrain = tf.train.AdamOptimizer(lr).minimize(Gloss, var_list=gvars) # train generator only\n",
    "\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on epoch 0\n",
      "on epoch 1\n",
      "on epoch 2\n",
      "on epoch 3\n",
      "on epoch 4\n",
      "on epoch 5\n",
      "on epoch 6\n",
      "on epoch 7\n",
      "on epoch 8\n",
      "on epoch 9\n",
      "on epoch 10\n",
      "on epoch 11\n",
      "on epoch 12\n",
      "on epoch 13\n",
      "on epoch 14\n",
      "on epoch 15\n",
      "on epoch 16\n",
      "on epoch 17\n",
      "on epoch 18\n",
      "on epoch 19\n",
      "on epoch 20\n",
      "on epoch 21\n",
      "on epoch 22\n",
      "on epoch 23\n",
      "on epoch 24\n",
      "on epoch 25\n",
      "on epoch 26\n",
      "on epoch 27\n",
      "on epoch 28\n",
      "on epoch 29\n",
      "on epoch 30\n",
      "on epoch 31\n",
      "on epoch 32\n",
      "on epoch 33\n",
      "on epoch 34\n",
      "on epoch 35\n",
      "on epoch 36\n",
      "on epoch 37\n",
      "on epoch 38\n",
      "on epoch 39\n",
      "on epoch 40\n",
      "on epoch 41\n",
      "on epoch 42\n",
      "on epoch 43\n",
      "on epoch 44\n",
      "on epoch 45\n",
      "on epoch 46\n",
      "on epoch 47\n",
      "on epoch 48\n",
      "on epoch 49\n",
      "on epoch 50\n",
      "on epoch 51\n",
      "on epoch 52\n",
      "on epoch 53\n",
      "on epoch 54\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init) \n",
    "    for e in range(rounds):\n",
    "        nbatch = mn.train.num_examples // batchsz\n",
    "        # train the network for 1 batch (there are # of Images/Bach_size batches in each epoch)\n",
    "        for i in range(nbatch):\n",
    "            batch = mn.train.next_batch(batchsz) # get the next batch of training data\n",
    "            batchimgs = batch[0].reshape((batchsz, 784))\n",
    "            batchimgs = batchimgs * 2 - 1 # adjust values for tanh activation, batch imgs are pre-normalized\n",
    "            batch_z = np.random.uniform(-1,1,size=(batchsz,100)) # generate random seed batch for faker\n",
    "            sess.run(jtrain, feed_dict={real_img: batchimgs, z: batch_z}) # train one batch on judge\n",
    "            sess.run(gtrain, feed_dict={z:batch_z}) # train one batch on faker\n",
    "        print(\"on epoch {}\".format(e))\n",
    "        samplez = np.random.uniform(-1,1,size=(1,100))  # make random seed vector for sampling\n",
    "        fakeimg = sess.run(gen(z, reuse=True, extra_layer=extra_layer), feed_dict={z: samplez}) # make fake images\n",
    "        samples.extend(fakeimg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''\n",
    "code to save and load results\n",
    "uncomment to (switch) save the appropriate sample vector file\n",
    "'''\n",
    "'''\n",
    "\n",
    "'''\n",
    "np.save('./samples_250', samples) \n",
    "res = np.load('./samples_250.npy') \n",
    "# # samples_250t_1.npy\n",
    "\n",
    "'''\n",
    "# np.save('./samples_250t_FASHION', samples) \n",
    "'''\n",
    "# res = np.load('./samples_250_FASHION_3.npy') \n",
    "\n",
    "# # samples_250_FASHION_3\n",
    "# # samples_250t_FASHION.npy\n",
    "\n",
    "loaded_samples = res  #samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos, imgs in enumerate(loaded_samples):\n",
    "    if pos%skip ==0 or pos==250-1 :\n",
    "            visualize_sample(imgs, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (wcd-ml)\n",
   "language": "python",
   "name": "wcd-ml-b2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
